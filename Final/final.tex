\documentclass[usletter]{article}
\usepackage[margin=1.5in]{geometry}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cancel}
\usepackage[mathscr]{eucal}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{enumitem}
\usepackage{graphicx}

\usepackage{scribe}

\newcommand {\namedset}[1]     {\ensuremath{\mathbb{#1}}}
\newcommand {\langset}[1]      {\ensuremath{\mathcal{#1}}}
\newcommand {\machine}[1]      {\ensuremath{\mathscr{#1}}}
\newcommand {\langfunc}        {\ensuremath{\mathfrak{L}}}
\newcommand {\namedlangset}[1] {\ensuremath{\textnormal{\textsc{#1}}}}
\newcommand {\family}[1]       {\ensuremath{\mathsf{#1}}}

\newcommand {\term}[1]      {\textit{#1}}
\newcommand {\namethm}[1]   {\term{#1} Theorem}
\newcommand {\usingthm}[1]  {(Using \namethm{#1})}

\newcommand {\reduce}[2]    {\ensuremath{\preceq_{#1}^{#2}}}
\newcommand {\complete}[2]  {\ensuremath{\reduce{#1}{#2}     %% ugly hack ahead!
                                         \hspace*{-4pt}\textnormal{-complete}}}

\newcommand {\indpar}[1]   {
  \par\leftskip=#1em
  \noindent\ignorespaces
}
\newenvironment{turing}[2] {
  \smallskip
  \indpar{2}
  \textit{Machine:} #1\\
  \textit{Input:} $#2$\\[5pt]
  \texttt{begin}
  \parskip=0pt
  \indpar{3}
}{
  \indpar{2}
  \texttt{end}
  \par\medskip
}

\newcommand {\ie}       {\textnormal{i.e. }}
\newcommand {\todo}[1]  {{\large \textbf{TODO: #1}}}
\newcommand {\field}[2] {\ensuremath{\namedset{F}_{#1}^{#2}}}

%%% Defaults for languages and machines
\newcommand {\langL}          {\langset{L}}
\newcommand {\machineM}       {\machine{M}}
%%%

%%% Complexity Classes
\newcommand {\PTime}  {\family{P}}
\newcommand {\PH}     {\family{PH}}
\newcommand {\IP}     {\family{IP}}
\newcommand {\NP}     {\family{NP}}
\newcommand {\RP}     {\family{RP}}
\newcommand {\BPP}    {\family{BPP}}
\newcommand {\PSPACE} {\family{PSPACE}}
\newcommand {\FPi}    {\family{\Pi}}
\newcommand {\FSigma} {\family{\Sigma}}
%%%

\begin{document}

\makeheader {Saswat Padhi} {March 18, 2016}
            {CS 281A -- Computability and Complexity}

\begin{enumerate}[labelsep=2.5em, label=\textbf{\arabic{enumi}}]
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Q1
  \item A language $\langL$ is \term{Turing-recognizable} if there is a Turing
        machine that halts if and only if the input is in \langL. Prove for an
        explicit language that it is not Turing-recognizable.
  %-----------------------------------------------------------------------------
  \begin{proof}[Intuition]
    We know that problem \namedlangset{Halt} is \term{undecidable}. So, at least
    one of \namedlangset{Halt}, or $\overline{\namedlangset{Halt}}$ is not
    Turing-recognizable.
    But \namedlangset{Halt} is. So, $\overline{\namedlangset{Halt}}$ must not be!
  \end{proof}
  \begin{proof}
    Consider the complement language of \namedlangset{Halt}:
    $$ \overline{\namedlangset{Halt}}
    = \{ \langle \alpha, x \rangle \mid
         \machineM_\alpha \text{ never halts on } x \} $$
    We would prove that $\overline{\namedlangset{Halt}}$ is not
    Turing-recognizable.

    We begin by demonstrating that \namedlangset{Halt} is indeed
    Turing-recognizable.
    \begin{claim}\namedlangset{Halt} is Turing-recognizable.\end{claim}
    \begin{proof}
      Consider a universal TM \machine{U}. \\
      By construction, \machine{U} halts on an input $\langle \alpha, x \rangle$
      if and only if $\machineM_\alpha$ halts on $x$; because \machine{U} just
      simulates $\machineM_\alpha$ with at most a logarithmic time overhead.

      Clearly, \machine{U} thus \term{recognizes} \namedlangset{Halt}.
    \end{proof}

    \begin{assumption}
      For the sake of contradiction; assume the existence of a TM \machine{V}
      which recognizes $\overline{\namedlangset{Halt}}$ i.e. \machine{V} halts if
      and only if $\machineM_\alpha$ never halts on $x$.
    \end{assumption}

    Now consider the following TM, which we can construct using \machine{U} and
    \machine{V}:
    \begin{turing}{\machine{H}}{\langle  \alpha, x \rangle}
      simulate next step of \machine{U} on $\langle  \alpha, x \rangle$,
      halt with $1$ if \machine{U} halted. \\
      simulate next step of \machine{V} on $\langle  \alpha, x \rangle$,
      halt with $0$ if \machine{V} halted. \\
      repeat the above again
    \end{turing}

    \begin{claim}
      The TM \machine{H} \term{decides} \namedlangset{Halt}.
    \end{claim}
    \begin{proof}
      It is easy to see that \machine{H} halts for all inputs. Consider any
      input $\langle \alpha, x \rangle$. Then one of the following must hold:
      \begin{itemize}
        \item $\machineM_\alpha$ halts on on $x$, then \machine{U} must
          eventually halt. \\
          Thus \machine{H} must halt with $1$.
        \item $\machineM_\alpha$ never halts on $x$, then (\machine{U} never
          halts and) \machine{V} must eventually halt. \\
          Thus \machine{H} must halt with $0$.
      \end{itemize}
      Therefore, \machine{H} must eventually halt, in any case.

      Further, \machine{H} halts with $1$ on input $\langle \alpha, x \rangle$
      if and only if \machine{U} halts on the same input i.e. $\machineM_\alpha$
      halts on $x$. In other words, \machine{H} decides \namedlangset{Halt}.
    \end{proof}

    Hence, we have shown that using \machine{V}; we can \term{decide}
    \namedlangset{Halt} -- which is \term{undecidable}! \\
    Then our assumption regarding the existence of such a TM \machine{V} which
    recognizes $\overline{\namedlangset{Halt}}$ must be false.

    Therefore, $\overline{\namedlangset{Halt}}$ is not
    \term{Turing-recognizable}.
  \end{proof}


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Q2
\item Give a polynomial-time algorithm for determining whether a given
      \term{2-CNF} formula is satisfiable.
  %-----------------------------------------------------------------------------
  \begin{proof}[Intuition]
    In case of \term{2-CNF}, assigning truth value to one literal in a
    clause, \textit{forces} a truth value on the other literal.
  \end{proof}
  \begin{proof}[Solution]
    Consider a \term{2-CNF} formula $\Delta$ defined on variables
    $x_1, x_2, \ldots, x_n$. \\
    We can rewrite any clause $(l_i \vee l_j)$, where $l_i$ and $l_j$ are
    literals; as $(\neg l_i \Rightarrow \l_j)$.

    Now consider a graph with each literal $l_k$ being a vertex and each edge
    being an \textit{implication}. For each clause $(l_i \vee l_j)$, we add
    edges $\overline{l_i} \longrightarrow l_j$ and
    $\overline{l_j} \longrightarrow l_i$, where $\overline{l_k}$ indicates the
    complement literal of $l_k$. So, construction of this graph can be done in
    time $O(p)$, where $p$ is the number of clauses in $\Delta$; but it is
    bounded by $O(n^2)$.

    Let's also represent a directed path over this graph from $l_i$ to $l_j$, as
    $l_i \rightsquigarrow l_j$. Since our graph edges model implications,
    $l_i \rightsquigarrow l_j$ would indicate $l_i \Rightarrow l_j$, due to the
    transitivity of implications.

    Note that a path $\overline{l_i} \rightsquigarrow l_i$ would force the
    assignment $l_i = 1$. Then we would face a contradiction for assigning a
    variable $x_i$ if:
    $$
      (x_i \rightsquigarrow \neg x_i) \wedge (\neg x_i \rightsquigarrow x_i)
      \qquad \text{as this would require,}
      \quad (x_i = 1) \wedge (\neg x_i = 1)
    $$

    We know that checking $l_i \rightsquigarrow l_j$ on a graph with $O(n)$
    vertices, is in polynomial-time. For example, a simple breadth-first-search
    would run in time $O(|V|+|E|) = O(n^2)$.

    So our algorithm could simply check the following: \\
    For each variable $x_k$ in $x_1, x_2, \ldots, x_n$; run two queries on the
    graph - check if $x_k \rightsquigarrow \neg x_k$ and
    $\neg x_k \rightsquigarrow x_k$. If both queries are satisfied for any
    variable $x_k$, then $\Delta \not\in \namedlangset{2-SAT}$. The algorithm
    would run in time $O(2 n \cdot n^2) = O(n^3)$. \\
    The soundness of this algorithm is a direct observation: if the algorithm
    finds some variable $x_k$ such that $x_k \rightsquigarrow \neg x_k$ and
    $\neg x_k \rightsquigarrow x_k$; then there exists no assignment for $x_k$,
    and thus $\Delta \not\in \namedlangset{2-SAT}$.

    \begin{claim}[Completeness]
      If $\Delta \not\in \namedlangset{2-SAT}$, the algorithm would eventually
      find some $x_i$ such that $x_i \rightsquigarrow \neg x_i$ and
      $\neg x_i \rightsquigarrow x_i$.
    \end{claim}
    \begin{proof}
      If $\Delta \not\in \namedlangset{2-SAT}$, then there must be at least one
      variable $x_i$ which can neither be assigned $0$, nor $1$; without
      violating at least one of the clauses.

      For the sake of contradiction, now assume that our algorithm does not
      detect at least one of these paths: $x_i \rightsquigarrow \neg x_i$ or
      $\neg x_i \rightsquigarrow x_i$ for this variable $x_i$. We will show that
      then there exists an assignment for $x_i$, which satisfies $\Delta$.

      For each variable $x_k$, we do the following:
      \begin{enumerate}
        \item if $x_k \rightsquigarrow \neg x_k$ was detected, assign $x_k = 0$.
        \item if $\neg x_k \rightsquigarrow x_k$ was detected, assign $x_k = 1$.
        \item if neither was detected, assign $x_k$ to $0$ or $1$ arbitrarily
      \end{enumerate}
      By our assumption, for no variable both cases (1) and (2) would be
      applicable.

      Now, since each edge of the graph corresponds to a clause in $\Delta$,
      this assignment is guaranteed to satisfy every clause, thus making it a
      satisfying assignment for $\Delta$!

      This contradicts the fact that $\Delta \not\in \namedlangset{2-SAT}$. \\
      The only reason could be our mistake in the assumption that our algorithm
      does not detect an inconsistency for variable $x_i$ --- which must be
      false.
    \end{proof}

    So we can decide if $\Delta \in \namedlangset{2-SAT}$, in time
    $O(n^2 + n^3) = O(n^3)$; and thus $\namedlangset{2-SAT} \in \PTime$.
  \end{proof}


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Q3
  \item Let \langL\ be any \NP-complete language. Prove that $\langL \in \BPP$,
        if and only if $\langL \in \RP$.
  %-----------------------------------------------------------------------------
  \begin{proof}
    Let \langL\ be an arbitrary \NP-complete language as stated. \\
    Since $\RP \subseteq \BPP$ and \namedlangset{SAT} is \complete{p}{} for \NP,
    it would be enough to prove that:
    $$
    \text{If } \namedlangset{SAT} \in \BPP
    \text{, then } \namedlangset{SAT} \in \RP
    $$
    Per assumption, there must exist a randomized polynomial-time TM \machine{B}
    which decides \namedlangset{SAT}, with error rate $\leq \frac{1}{3}$, with
    $O(|x|^c)$ random bits for any input $x$.

    We construct the following randomized, polynomial-time TM \machine{R}, which
    \textit{uses} \machine{B} and decides \namedlangset{SAT}:
    \begin{turing}
          {\machine{R}}
          {\langle x, r \rangle
            \hfill x \text{ has } n \text{ variables: } v_0, v_2 \ldots v_{n-1}
            \quad \text{and} \quad |r| = n|x|^{c+1}}
      for $i$ = $0$ to $n-1$:
        \indpar{4}
        construct $x_i$ as $x$ with variable $v_i$ replaced by $0$ \\
        construct $r_i$ as the substring of $r$ within
          $[i \cdot |x|^{c+1}, (i+1) \cdot |x|^{c+1}]$ \\
        simulate \machine{B} on input $\langle x_i, r_i \rangle$ \\
        if simulation halts with $1$, assign $v_i \mapsto 0$ else
          assign $v_i \mapsto 1$
      \indpar{3}
      if the assignment to $v_0, v_2, \ldots , v_{n-1}$ satisfies $x$,
        output $1$ else output $0$
    \end{turing}

    The soundness of \machine{R} is a direct observation: due to the final check
    \machine{R} would never accept an $x \not\in \namedlangset{SAT}$. Although,
    \machine{R} might mistakenly declare an $x \in \namedlangset{SAT}$ as
    \namedlangset{UNSAT}; due to the fact that the particular assignment of
    $v_0, v_2, \ldots , v_{n-1}$ might not satisfy $x$.

    Next determine the false negative rate for \machine{R}; which is required to
    be $\leq \frac{1}{3}$, for \machine{R} to establish that
    $\namedlangset{SAT} \in \RP$. Clearly \machine{R} be incorrect, if at least
    $1$ of the simulations of \machine{B} turns out to be incorrect. The
    probability of this happening is $\frac{n}{3}$, which is too high!

    But as discussed in class, using the Chernoff bound, we know that the
    majority of $k$ independent runs of \machine{B} would be wrong with
    probability only $2e^{-k/18}$. For simplicity of calculations, let us
    over-estimate this error as $e^{-k/2}$ (note that $e^{-k/2} > 2e^{-k/18}$
    for all integral values of $k > 0$).\\
    Now if we use $k$ runs instead of one run per variable, we would have error
    rate of \machine{R} $\leq \frac{n}{e^{k/2}}$. This quantity can be made
    $\leq \frac{1}{3}$ as required to be in \RP; by choosing a suitable $k$,
    specifically $k \geq 2\log(3n)$. The new construction of \machine{R}
    would look like:
    \begin{turing}
          {\machine{R}}
          {\langle x, r \rangle
            \hfill x \text{ has } n \text{ variables: } v_0, v_2 \ldots v_{n-1}
            \quad \text{and} \quad |r| = 2n\log(3n)|x|^{c+1}}
      $k := 2\log(3n)$ \\
      for $i$ = $0$ to $n-1$:
        \indpar{4}
        construct $x_i$ as $x$ with variable $v_i$ replaced by $0$ \\
        $c_0 := 0 \quad c_1 := 0$ \\
        for $j$ = $0$ to $k$:
          \indpar{5}
          construct $r_i$ as the substring of $r$ within
            $[(ik+j-1) \cdot |x|^{c+1}, (ik+j) \cdot |x|^{c+1}]$ \\
          simulate \machine{B} on input $\langle x_i, r_i \rangle$ \\
          if simulation halts with $1$, increment $c_0$ else increment $c_1$
        \indpar{4}
        if $c_0 > c_1$, $v_i \mapsto 0$ else assign $v_i \mapsto 1$
      \indpar{3}
      if the assignment to $v_0, v_2, \ldots , v_{n-1}$ satisfies $x$,
        output $1$ else output $0$
    \end{turing}

    Note that, \machine{R} still runs in polynomial-time. Specifically it is
    $O(n\log{n}T_\machine{B}(n))$; where $T_\machine{B}$ is the running time of
    \machine{B}; since computing $k$, constructing $x_i$ and $r_i$ and checking
    if the assignments satisfy $x$ --- can all be done in smaller polynomials of
    $n$. Now, since we know that \machine{B} is an algorithm in \BPP,
    $T_\machine{B}$ is polynomial-time; which establishes that \machine{R} runs
    in polynomial-time.

    As \machine{R} now has a false-negative rate $\leq \frac{n}{e^{k/2}} \leq
    \frac{n}{3n} = \frac{1}{3}$; it establishes that
    $\namedlangset{SAT} \in \RP$; starting with the assumption that
    $\namedlangset{SAT} \in \BPP$. \\
    And since $\namedlangset{SAT} \in \RP$, we would have the same for any
    \NP-complete problem.
  \end{proof}
  \begin{remark}
    The above proof implies : $\NP \subseteq \RP$. But in fact $\NP = \RP$! \\
    This is because $\RP \subseteq \NP$, as discussed in class -- a problem
    being in \RP, shows that it has at least \textit{exponentially} many
    certificates; which is a subset of problems with at least \textit{one}
    certificate (\NP).
  \end{remark}


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Q4
  \item Prove that there is a real number $0 < p < 1$ with the following
        property: given access to a coin with bias $p$, a randomized Turing
        machine can decide an undecidable language in polynomial time. By
        \textit{access} to the coin is meant the ability to receive any desired
        number of random bits $X_1, X_2, X_3 \ldots \in \{0,1\}$, each being an
        independent random variable with expectation $p$.
  %-----------------------------------------------------------------------------
  \begin{proof}[Intuition]
    Not all real numbers are computable \ldots\ $p$ is one of them.
  \end{proof}
  \begin{proof}
    First, we show that any \textit{language} can be encoded as a real number. A
    simple algorithm would be:
    \begin{align*}
      \machine{R}(\langL) &= 0. b_1 b_2 b_3 \ldots \\
      \text{where } b_i &= 1 \text{ if } w_i \in \langL, 0 \text{ otherwise} \\
      w_i &= i^{th} \text{ string in } \{0,1\}^*
    \end{align*}
    Note that the $i^{th}$ string here indicates that, all the strings in
    $\{0,1\}^*$ are ordered by increasing value when interpreted as integers.

    We observe the following properties of \machine{R}:
    \begin{itemize}
      \item $\machine{R}(\langL)$ is a valid real number represented in binary.
      \item $\machine{R}(\langL)$ has infinite digits (since $\{0,1\}^*$ is an
            infinite set).
      \item And \machine{R} is always unambiguously \textit{exists} all
            $\langL \subseteq \{0,1\}^*$.
      \item By the bit-wise construction of $\machine{R}(\langL)$, it is clear
            that $\forall \langL \subseteq \{0,1\}^* :
            0 \leq \machine{R}(\langL) \leq 1$
    \end{itemize}

    Now let's try to define $\machine{R}(\namedlangset{Halt})$.
    \begin{align*}
      \machine{R}(\namedlangset{Halt}) &= 0. b_1 b_2 b_3 \ldots \\
      \text{where } b_i &= 1 \text{ if } \machineM_{\alpha_i}
        \text{ halts on } x_i, 0 \text{ otherwise} \\
      \langle \alpha_i, x_i \rangle &= i^{th} \text{ string in } \{0,1\}^*
        \text{ decoded to a tuple}
    \end{align*}
    As argued above, $\machine{R}(\namedlangset{Halt})$ is well-defined.

    We show that if a randomized TM is given access to a coin with bias
    $\machine{R}(\namedlangset{Halt})$ is known; then it can decide
    \namedlangset{Halt} in polynomial-time. The TM just needs to know the
    $i^{th}$ digit of $\machine{R}(\namedlangset{Halt})$. \\
    Consider the following randomized Turing machine:
    $\machine{R}(\namedlangset{Halt})$:
    \begin{turing}{$\machine{RM}_\namedlangset{Halt}$}
                  {\langle \alpha, x \rangle}
      compute the index $i$ of the string $w_i = \langle \alpha, x \rangle$ in
        $\{0,1\}^*$ \\
      extract the $i^{th}$ bit of $\machine{R}(\namedlangset{Halt})$ by repeated
        flips of the available coin \\
      return $b_i$
    \end{turing}
    Computing the index $i$ can be easily done in polynomial time -- simply
    evaluating the binary string $\langle \alpha, x \rangle$ to an integer.

    Extracting the $i^{th}$ bit of $\machine{R}(\namedlangset{Halt})$ can also
    be achieved in polynomial-time --- we use Chernoff bound to approximate the
    value of $\machine{R}(\namedlangset{Halt})$ to arbitrary precision $\delta$,
    by multiple tosses of the coin. \\
    (Due to lack of time, I could not formalise this part.)

    Thus, given a coin with bias $\machine{R}(\namedlangset{Halt})$,
    we can decide \namedlangset{Halt}, in polynomial-time.
  \end{proof}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Q5
  \item Prove that the polynomial hierarchy collapses if \namedlangset{TQBF} has
        a polynomial-time \textit{randomized} algorithm.
  %-----------------------------------------------------------------------------
  \begin{proof}
    If \namedlangset{TQBF} has a polynomial-time randomized algorithm,
    $\namedlangset{TQBF} \in \BPP$.

    Due to \namethm{Sipser-G\'{a}cs-Lautemann}\cite{Lautemann1983}:
    $\BPP \subseteq \FSigma_2 \cap \FPi_2$.
    Then, $\namedlangset{TQBF} \in \FSigma_2$.

    But we know that \namedlangset{TQBF} is \complete{p}{} for \PSPACE. So, any
    problem $\langset{P} \in \PSPACE$ can be reduced to \namedlangset{TQBF} in
    polynomial-time. Since $\PH \subseteq \PSPACE$, the reduction also holds for
    any problem $\langset{P} \in \PH$.

    But since we have assumed that $\namedlangset{TQBF} \in \FSigma_2$, we have
    actually shown that: \\
    $$ \forall \langset{P} \in \PH: \langset{P} \in \FSigma_2 $$
    In other words, the polynomial hierarchy \PH\ collapses to the $2^\text{nd}$
    level.
  \end{proof}


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Q6
  \item Two players take turns choosing vertices from a directed graph $G$, such
        that the $i^\text{th}$ vertex chosen $v_i$, is distinct from $v_0, v_1,
        v_2, ..., v_{i-1}$ and reachable from $v_{i-1}$ by an edge. When a
        player has no vertices to choose from, the game ends and that player
        loses. On input a directed graph $G$, give a polynomial-space algorithm
        for deciding whether the player who starts the game can force a win,
        regardless of his opponent's choices. \\
        \textit{Comment:} This problem makes it clear why \PSPACE\ is often
        regarded as the class of adversarial games with two players.
  %-----------------------------------------------------------------------------
  \begin{proof}[Intuition]
    For the first player to be able to force a win, at any point in the game --
    there should \textit{exist} a move for him/her; which allows him to force a
    win, \textit{for all} possible moves of the second player. That sounds like
    \namedlangset{TQBF}!
  \end{proof}
  \begin{proof}[Solution]
    Consider the following recursive algorithm, similar to one for
    \namedlangset{TQBF}:

    \begin{minipage}{\linewidth}
    \begin{function}[H]
      \caption{FirstPlayerWins()}
      \SetKwFunction{FirstPlayerWins}{FirstPlayerWins}
      \SetKwFunction{Union}{Union}\SetKwFunction{RemoveVertices}{RemoveVertices}

      \KwIn   {A graph $G$ \tcc*[r]{$V_G$ and $E_G$ are vertex and edge sets}}
      \KwOut  {$0$ if the first player can force a win, $1$ otherwise}

      \Begin{
        \For{\textnormal{each vertex} $x_i \in V_G$ \tcc*[r]{P1 chooses $x_i$}}{
          $flag_{won} \longleftarrow 1$ \;
          \For{\textnormal{each vertex $x_j$ s.t. $(x_i, x_j) \in E_G$}
            \tcc*[r]{P2 chooses $x_j$}
          } {
            \If{$\FirstPlayerWins{\RemoveVertices{$G, \{x_i, x_j\}$}} = 0$}{
              $flag_{won} \longleftarrow 0$ \tcc*[r]{could not force win}
              break
            }
          }
          \tcc*[r]{if player won for all choices of $x_j$}
          \lIf{$flag_{won} = 1$}{\Return{$1$}}
          \tcc*[r]{otherwise try next $x_i$}
        }
        \Return{$0$}
      }
    \end{function}
    \end{minipage}

    The call to function \texttt{RemoveVertices}, simply removes all supplied
    vertices from the provided graph, along with all of their incoming and
    outgoing edges.

    Soundness and completeness of \texttt{FirstPlayerWins} are directly observed
    since the algorithm explores all possible moves for both players.

    We now show that \texttt{FirstPlayerWins} uses polynomial-space. \\
    Let the space usage of \texttt{FirstPlayerWins} as a function of the
    argument graph $G$ be $S(G)$. Since during each recursive call, we only
    store $\langle G, x_i, x_j \rangle$ on the stack;
    \begin{align*}
      S(G) &= S(G') + |G| + 2
           & G' = \text{\tt RemoveVertices}(G, \{x_i, x_j\}) \\
           &= S(G') + O(n^2) & n = |V_G |
    \end{align*}
    Since we remove $2$ vertices from $G$ to obtain $G'$, the maximum depth of
    recursion can be $\frac{n}{2}$. Now, unrolling the recursive relation
    gives us $S(G) = O(n^3)$.

    Therefore \texttt{FirstPlayerWins} runs in space, polynomial in size of
    the input graph $G$.
  \end{proof}


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Q7
  \item Prove that increasing the completeness parameter in the definition of
        \IP\ from $\frac{2}{3}$ to $1$, does not change this complexity class.
        What class do we get if we keep the completeness unchanged but increase
        the soundness from $\frac{2}{3}$ to $1$?
  %-----------------------------------------------------------------------------
  \begin{proof}[Intuition]
    The completeness claim is easy to see because $\IP = \PSPACE$ as discussed
    in class. For the soundness = $1$ case, the verifier would behave like a
    deterministic verifier; which should essentially reduce the system to \NP.
  \end{proof}
  \begin{proof}
    Fist we show that the class \IP\ is unaffected by an increase to the
    completeness parameter. Note that, since we have $\IP = \PSPACE$ as shown in
    the lecture:
    $$
    \forall \langL \in \IP : \exists f :
    (\forall x : x \in \langL \Longleftrightarrow f(x) \in \namedlangset{TQBF})
    $$
    where $f$ is a polynomial-time transformation from an instance of \langL\ to
    \namedlangset{TQBF}.

    In the lecture, we discussed an interactive system for \namedlangset{TQBF}
    with completeness = $1$; then an interactive system for \langL\ with
    completeness = $1$ immediately follows due to the transformation.

    Now we prove the following theorem for increasing soundness of \IP\ systems
    to $1$.
    \begin{theorem}
      If $\langL \in \IP$ has a proof system with a soundness of $1$; then
      $\langL \in \NP$.
    \end{theorem}
    \begin{proof}
      If we have a proof system for \langL, with soundness = $1$; then we show a
      way to build an \NP\ algorithm for \langL. Let's call the prover in the
      system as \machine{P} and the verifier as \machine{V}. We show below that
      \machine{V} is also a verifier in \NP.

      First, consider any instance $x \not\in \langL$. \\
      Since the system is sound; \machine{P} can never force \machine{V} to
      accept, regardless of the random bits chosen. This is as required by \NP.

      Now, consider any instance $x \in \langL$. \\
      By the previous theorem; there is an \IP\ system for \langL, which must
      accept $x$ with probability $1$. This indicates that, \textit{regardless}
      of the random bits chosen, the system accepts $x$ -- in other words,
      \machine{P} always ensures acceptance of $x$ on \machine{V}. \\
      Therefore, we can simply fix an arbitrary random string for \machine{V};
      and expect all the answers from \machine{P} together as a certificate ---
      such a certificate would exist as argued. The verifier thus behaves as a
      verifier in \NP\ now!
    \end{proof}

    Thus we have shown that, increasing the completeness parameter of \IP\ to
    $1$ does not change the complexity class. But increasing the soundness
    parameter to $1$ reduces its power, down to \NP.
  \end{proof}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Q8
  \item Prove that with Hadamard coding, it is in general impossible to recover
        the original codeword, if the received transmission is corrupted in
        $25\%$ or more of the coordinates.
  %-----------------------------------------------------------------------------
  \begin{proof}[Intuition]
    Error correction essentially finds the \textit{closest} valid Hadamard code,
    for a corrupted code. We need to find a limit on this, beyond which the
    \textit{closest} cannot be uniquely defined.
  \end{proof}
  \begin{proof}
    Let's assume that for any $2$ strings of length $n$, their Hadamard codes
    have a distance of at least $\Delta^n_{min}$:
    $$
    \forall C_i, C_j \in \field{2}{n} : \Delta(C_i, C_j) \geq \Delta^n_{min}
    \qquad \text{if } C_i, C_j \text{ valid Hadamard codewords}
    $$
    Let's name $C_x$ and $C_y$ as the codewords which are separated by this
    minimum distance.

    Now, let's say we allow corruption of $k \geq \frac{1}{2}\Delta^n_{min}$
    coordinates. \\
    Then a corrupted codeword $C'_x$ would be within distance $k$ of not only
    $C_x$, but also $C_y$ i.e.
    $$
    \Delta(C'_x, C_x) = k \wedge \Delta(C'_x, C_y) \leq k
    $$
    At this point, it would be impossible for any algorithm to predict if $C'_x$
    was formed by correction of $C_x$ or $C_y$.

    Thus, it's clear that we can only correct corruption of
    $< \frac{1}{2}\Delta^n_{min}$ coordinates. \\

    \begin{claim}
      For Hadamard codes, $\Delta^n_{min} = 2^{n-1}$
    \end{claim}
    \begin{proof}
      Consider $2$ Hadamard codewords $C_x$, $C_y$. If they agree at index $u$:
      \begin{alignat*}{2}
          & \quad C_x(u) - C_y(u) &= 0 \\
        \Rightarrow & \quad \langle x, u \rangle - \langle y, u \rangle &= 0 \\
        \Rightarrow & \quad \langle x - y, u \rangle &= 0
      \end{alignat*}
      For arbitrary $C_x$ and $C_y$, this would hold for half the indices. \\
      Since the length of the codewords is $2^n$, half the number of indices is
      $2^{n-1}$.
    \end{proof}

    Therefore, we can only correct at most $25\%$, or $\frac{2^{n}}{4}$ bits
    corruption of Hadamard codewords of length $2^{n}$.
  \end{proof}

\end{enumerate}

\bibliographystyle{abbrv}
\bibliography{final}

\end{document}
